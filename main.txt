!pip install -U -q PyDrive ## you will have install for every colab session
!pip install wordcloud
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# 1. Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

#---------------------------------------------

from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import numpy as np

from google.colab import drive 
drive.mount('/content/drive') #This way you can access all files on your drive
#!ls "/content/drive/My Drive/Colab Notebooks" 

#----------------------------------------------


# link = 'https://drive.google.com/open?id=119WXujCb5KzFVpxHRPZmxHtsS2ijzNIY'
# fluff, id = link.split ('=')
# print (id)

# downloaded = drive.CreateFile({'id':id})
# downloaded.GetContentFile('glove_s50.txt')

# link = 'https://drive.google.com/open?id=1XrKtmemSI-Hw_XRQsnMDTrDp3F0IEcEF'
# fluff, id = link.split ('=')
# print (id)

# downloaded_test = drive.CreateFile({'id':id})
# downloaded_test.GetContentFile('arquivo')

# input("arquivo lido")









def character_remove(old, to_remove):
    new_string = old
    for x in to_remove:
        new_string = new_string.replace(x, '')
    return new_string


  

def vectorize(text, model, feature_names):
  """Receives the raw text and the glove model to perform the word-for-word vectorization 
and returns a 50-dimensional vector representing the text through the vector mean of 
all words "Found in the glove"". """ 
  
  stopWords=[]
  vector = 0 
  index = 0
  for line in text:
    aux = line.split()
    for word in aux:
      try:
          if index == 0:
            word = character_remove(word, "!@#$%&.,!?+-=")
            if(word in feature_names):
              vector = list(model[word])
              index += 1
          else:
            if len(list(model[word])) != 50:
              print("Diferente de 50")
              print(word)
              print(list(model[word]))
            
             
            if(word in feature_names):
              vector = np.add(vector,list(model[word]))
              index += 1
      except (KeyError):
        continue
  
  
  vector = np.true_divide(vector,index)
  return vector



def load_text(name):
  #Will load the text from the file path of your drive
  file = open(name,"r")
  lines_text = file.readlines()
  file.close()
  
  return lines_text



def loadGloveModel(file_location):
  """Receives the location of the glove file with the word vectors and returns 
an matrix (n, 50) in which n is the number of existing words. You can search any vector using the key that is the word itself . """
  print ("Loading Glove Model\n")
  
  text_file = open(file_location, "r")
  content = text_file.readlines()
  text_file.close()
  
  
  model = {}
  print("runing...")
  i = 0
  try:
    for line in content:
      i += 1
      splitLine = line.split()
      word = splitLine[0]
      embedding = np.array([float(val) for val in splitLine[1:]])
      model[word] = embedding
  except ValueError:
    print(i)
    print(content[i-1])
    print(len(content[i-1]))

    
  print("finalized!\n")
  return model






def main():
    """Função principal da aplicação.
    """
    
    print("creating vocabulary")
    vetor = []
    for i in range(184):
      file = open("/content/drive/My Drive/TextosYoutubers/anderson/track_"+str(i+1)+".txt","r")
      vetor.append(file.read())
      file.close()
    cv=CountVectorizer(max_df=0.35,max_features=12000) #max_features=10000 ordena de acordo com a grandeza no corpus 
    response  = cv.fit_transform(vetor)
    feature_names = cv.get_feature_names()
    print("finished vocabulary")
    
    #You should use the path to your file directory
    model= loadGloveModel("/content/drive/My Drive/Colab Notebooks/glove_s50.txt")
    matriz = []
    #For a hundred texts I use the path path, read and vectorize the text and return the array of vectors that result from each one
    for i in range(1,185):
      name_text = "/content/drive/My Drive/TextosYoutubers/anderson/track_"+str(i)+".txt"
      text = load_text(name_text)
      matriz.append(vectorize(text,model, feature_names))


    #Elbow method. This method plots the sum of squares errors according to the number of iterated clusters in the matrix
    wcss = [] 
    for i in range(1, 6):
        kmeans = KMeans(n_clusters = i, init = 'k-means++')
        kmeans.fit(matriz)
        print (i,kmeans.inertia_)
        wcss.append(kmeans.inertia_)  
    plt.plot(range(1, 6), wcss)
    plt.title('O Metodo Elbow')
    plt.xlabel('Numero de Clusters')
    plt.ylabel('WSS') #within cluster sum of squares
    plt.show()



    #Clustering
    kmeans = KMeans(n_clusters = 5, init = 'k-means++')
    kmeans.fit(matriz) # efetua a clusterizacao fit_transform
    #print(kmeans.fit_transform(tokenizer)) // retorna a distancia de cada ponto em relacao aos centroides de cada cluster
    #kmeans.cluster_centers_ #  mostra os centroides 
    #print(len(kmeans.labels_) #mostra o centroide associado a cada token




    y_km = kmeans.fit_predict(matriz) # Compute cluster centers and predict cluster index for each sample 
    print(y_km,len(y_km))
    #plotting the graph
    myarray = np.asarray(matriz)
    plt.scatter(myarray[:, 0], myarray[:,1], s = 1500, c = kmeans.labels_)
    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'red',label = 'Centroids')
    plt.title('Clusters and Centroids')
    plt.legend()

    plt.show()

    #another graph
    plt.scatter(myarray[y_km == 0, 0],myarray[y_km == 0, 1],s=50, c='lightgreen',marker='s', edgecolor='black',label='cluster 1')
    plt.scatter(myarray[y_km == 1, 0],myarray[y_km == 1, 1],s=50, c='orange',marker='o', edgecolor='black',label='cluster 2')
    plt.scatter(myarray[y_km == 2, 0],myarray[y_km == 2, 1],s=50, c='lightblue',marker='v', edgecolor='black',label='cluster 3')
    plt.scatter(kmeans.cluster_centers_[:, 0],kmeans.cluster_centers_[:, 1],s=250, marker='*',c='red', edgecolor='black',label='centroids')
    plt.legend(scatterpoints=1)
    plt.grid()
    plt.show()
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    #Wordcloud
    cloudsTotal = 5
    clouds_rum = 0
    while (clouds_rum < cloudsTotal ):  
      text = 0
      textResult = 0 
      aux = 0
      y_km = list(map(int,y_km))
      for index,value in enumerate(y_km):
        if value == clouds_rum and aux == 0:
          textResult = open("/content/drive/My Drive/TextosYoutubers/anderson/track_"+str(index + 1)+".txt",'r').read()
          aux += 1

        elif value == clouds_rum and aux != 0:
          text = open("/content/drive/My Drive/TextosYoutubers/anderson/track_"+str(index + 1)+".txt",'r').read()
          textResult += " " + text
          aux += 1

      text_second = ""
      for x in textResult.split():
        if x in feature_names:
          text_second += " " + x
      print(text_second)
      #wordcloud = WordCloud(max_font_size=120,width = 1600, height = 600).generate(textResult)
      wordcloud = WordCloud(max_font_size=120,width = 1600, height = 600).generate(text_second)
      plt.figure(figsize=(16,9))
      plt.imshow(wordcloud)
      plt.axis("off")
      plt.show()
      print(aux)
        
      clouds_rum += 1
        
 
    
        


if __name__ == "__main__":
    main()
